{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import sklearn\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_openml, fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.datasets import fetch_covtype\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from torchvision.transforms.functional import normalize\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([5.5, 3])\n",
    "print(x)\n",
    "\n",
    "x = torch.zeros(2, 4)\n",
    "print(x)\n",
    "\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.ones(5,3)\n",
    "z = x+y\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(1)\n",
    "print(a)\n",
    "print(a.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = z.numpy()\n",
    "print(b)\n",
    "\n",
    "c = torch.from_numpy(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scalar derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(1., requires_grad = True)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.exp(a**2*torch.cos(a))\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = 1.\n",
    "print(-aa * np.exp(aa**2 * np.cos(aa)) * (aa * np.sin(aa) - 2 * np.cos(aa)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aten = torch.rand(2,2)\n",
    "print(aten)\n",
    "print(aten*aten)\n",
    "print(aten[0,0]*aten[0,0])\n",
    "print(aten.mm(aten))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aten = torch.rand(2,2,requires_grad = True)\n",
    "print(aten)\n",
    "bten = torch.exp(aten**2*torch.cos(aten))\n",
    "bten.backward(torch.Tensor([[1.,1.],[1.,1.]]))\n",
    "print(aten.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaten = aten[1,0].item()\n",
    "print(-aaten * np.exp(aaten**2 * np.cos(aaten)) * (aaten * np.sin(aaten) - 2 * np.cos(aaten)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jacobians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1.], requires_grad = True)\n",
    "b = torch.tensor([2.], requires_grad = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = a**2*b\n",
    "c2 = b*torch.cos(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.cat((c1,c2))\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for col in ([1.,0.],[0.,1.]):\n",
    "    c.backward(torch.tensor(col), retain_graph = True)\n",
    "    print(a.grad)\n",
    "    print(b.grad)\n",
    "    a.grad.data.zero_() # Otherwise the next pass sums with the previous one\n",
    "    b.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = 1.\n",
    "bb = 2.\n",
    "dc1daa = 2*aa*bb\n",
    "dc1dbb = aa**2\n",
    "dc2daa = -bb*np.sin(aa)\n",
    "dc2dbb = np.cos(aa)\n",
    "print([[dc1daa,dc1dbb],[dc2daa,dc2dbb]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression from Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xb,yb = make_blobs(n_samples = 250, n_features = 2,\n",
    "                   centers = [[1.3,1],[0.5,2.4]],\n",
    "                  cluster_std = 0.45, shuffle = True, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=5984,penalty='none').fit(Xb, yb)\n",
    "print(clf.coef_)\n",
    "print(clf.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = clf.predict_proba(Xb)[:,1]\n",
    "print(ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "log_loss(yb,ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = .02\n",
    "x_min, x_max = Xb[:, 0].min() - 1, Xb[:, 0].max() + 1\n",
    "y_min, y_max = Xb[:, 1].min() - 1, Xb[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "cmap_light = ListedColormap(['cyan', 'orange', 'lightgreen'])\n",
    "\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.figure()\n",
    "plt.pcolormesh(xx, yy, Z, cmap=cmap_light,shading='auto')\n",
    "\n",
    "plt.plot(Xb[:, 0][yb==0], Xb[:, 1][yb==0], \"bo\")\n",
    "plt.plot(Xb[:, 0][yb==1], Xb[:, 1][yb==1], \"ro\")\n",
    "\n",
    "plt.axis('tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom-written single layer neural network version of logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(Xb)\n",
    "y = torch.from_numpy(yb).unsqueeze(0).T\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show torch-based loss calculation on sklearn's results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = torch.from_numpy(clf.coef_.T)\n",
    "bs = torch.from_numpy(clf.intercept_).unsqueeze(0)\n",
    "y_s = 1./(1. + torch.exp(-(X.mm(ws)+bs)))\n",
    "loss_s = (- y * torch.log(y_s) - (1.-y) * torch.log(1. - y_s)).mean()\n",
    "loss_elements = (- y * torch.log(y_s) - (1.-y) * torch.log(1. - y_s))\n",
    "print(loss_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement gradient descent using torch's autodiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "indim = 2\n",
    "outdim = 1\n",
    "dtype = torch.double\n",
    "\n",
    "w = torch.randn(indim,outdim,dtype=dtype,requires_grad=True)\n",
    "b = torch.randn(1,outdim,dtype=dtype,requires_grad=True)\n",
    "learning_rate = 5e-2\n",
    "\n",
    "for t in range(50000):\n",
    "    y_pred = 1./(1. + torch.exp(-(X.mm(w)+b)))\n",
    "    loss = (- y * torch.log(y_pred) - (1.-y) * torch.log(1. - y_pred)).mean()\n",
    "    \n",
    "    if t % 1000 == 999:\n",
    "        print(\"Step {}: Loss = {}\".format(t,loss.item()))\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        \n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = w\n",
    "bs = b\n",
    "y_s = 1./(1. + torch.exp(-(X.mm(ws)+bs)))\n",
    "loss_s = (- y * torch.log(y_s) - (1.-y) * torch.log(1. - y_s)).mean()\n",
    "loss_elements = (- y * torch.log(y_s) - (1.-y) * torch.log(1. - y_s))\n",
    "print(loss_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "h = .02  # step size in the mesh\n",
    "x_min, x_max = Xb[:, 0].min() - 1, Xb[:, 0].max() + 1\n",
    "y_min, y_max = Xb[:, 1].min() - 1, Xb[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "# Flatten the grid points, align them as columns, and predict them\n",
    "Xgrid = torch.from_numpy(np.c_[xx.ravel(), yy.ravel()])\n",
    "y_round = (1./(1. + torch.exp(-(Xgrid.mm(w)+b)))).round()\n",
    "\n",
    "# Create color maps\n",
    "from matplotlib.colors import ListedColormap\n",
    "cmap_light = ListedColormap(['cyan', 'orange', 'lightgreen'])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = y_round.reshape(xx.shape).detach().numpy()\n",
    "\n",
    "plt.figure()\n",
    "plt.pcolormesh(xx, yy, Z, cmap=cmap_light,shading='auto')\n",
    "\n",
    "plt.plot(Xb[:, 0][yb==0], Xb[:, 1][yb==0], \"bo\")\n",
    "plt.plot(Xb[:, 0][yb==1], Xb[:, 1][yb==1], \"ro\")\n",
    "\n",
    "plt.axis('tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression using some built-in models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indim = 2\n",
    "outdim = 1\n",
    "X = torch.from_numpy(Xb).float()\n",
    "y = torch.from_numpy(yb).unsqueeze(0).T.float()\n",
    "\n",
    "logreg = torch.nn.Sequential(\n",
    "    torch.nn.Linear(indim,outdim,bias=True),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "learning_rate = 5e-2\n",
    "optimizer = torch.optim.SGD(logreg.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(50000):\n",
    "    y_pred = logreg(X)\n",
    "    loss = loss_fn(y_pred,y)\n",
    "    \n",
    "    if t % 1000 == 999:\n",
    "        print(\"Step {}: Loss = {}\".format(t,loss.item()))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = logreg[0].weight.data.T\n",
    "b = logreg[0].bias.data.T\n",
    "ws = w\n",
    "bs = b\n",
    "y_s = 1./(1. + torch.exp(-(X.mm(ws)+bs)))\n",
    "loss_s = (- y * torch.log(y_s) - (1.-y) * torch.log(1. - y_s)).mean()\n",
    "loss_elements = (- y * torch.log(y_s) - (1.-y) * torch.log(1. - y_s))\n",
    "print(loss_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense Multilayer Neural Network Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression on the California housing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fetch_california_housing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-062b90ea6104>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_california_housing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_X_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size = 0.2,\n\u001b[1;32m      3\u001b[0m                                     shuffle = True, random_state = 42)\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fetch_california_housing' is not defined"
     ]
    }
   ],
   "source": [
    "X_reg, y_reg = fetch_california_housing(return_X_y=True)\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size = 0.2,\n",
    "                                    shuffle = True, random_state = 42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_reg)\n",
    "X_train_reg = scaler.transform(X_train_reg)\n",
    "X_test_reg = scaler.transform(X_test_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train_reg).float()\n",
    "X_test = torch.from_numpy(X_test_reg).float()\n",
    "y_train = torch.from_numpy(y_train_reg).float().unsqueeze(1)\n",
    "y_test = torch.from_numpy(y_test_reg).float().unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "bs = 32\n",
    "train_ds = TensorDataset(X_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs)\n",
    "test_ds = TensorDataset(X_test, y_test)\n",
    "test_dl = DataLoader(test_ds, batch_size=len(test_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indim = 8\n",
    "hiddim = 10\n",
    "outdim = 1\n",
    "\n",
    "learning_rate = 2e-4\n",
    "epochs = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(indim,hiddim,bias=True),\n",
    "    torch.nn.ReLU(),\n",
    "    # torch.nn.Linear(hiddim,hiddim,bias=True),\n",
    "    # torch.nn.ReLU(),\n",
    "    # torch.nn.Linear(hiddim,hiddim,bias=True),\n",
    "    # torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hiddim,outdim,bias=True),\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss() # For regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = len(train_dl.dataset)\n",
    "for epoch in range(epochs):\n",
    "    for batch, (X, y) in enumerate(train_dl):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch % 10 == 0) and (batch == 0):\n",
    "            print(\"Epoch {} Batch {}: Training Loss = {}\".format(epoch,batch,\n",
    "                                                        loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for X, y in test_dl:\n",
    "        pred = model(X)\n",
    "        loss_value = loss_fn(pred, y).item()\n",
    "        print(\"Test Loss = {}\".format(loss_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification on the forest covertype dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_clf, y_clf = fetch_covtype(return_X_y=True)\n",
    "y_clf = y_clf - 1 # Need to change the classification categories to be indexed at 0\n",
    "X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(X_clf, y_clf, test_size = 0.2,\n",
    "                                    shuffle = True, random_state = 42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_clf)\n",
    "X_train_clf = scaler.transform(X_train_clf)\n",
    "X_test_clf = scaler.transform(X_test_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train_clf).float()\n",
    "X_test = torch.from_numpy(X_test_clf).float()\n",
    "y_train = torch.from_numpy(y_train_clf).long()#.unsqueeze(1)\n",
    "y_test = torch.from_numpy(y_test_clf).long()#.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 32\n",
    "train_ds = TensorDataset(X_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs)\n",
    "test_ds = TensorDataset(X_test, y_test)\n",
    "test_dl = DataLoader(test_ds, batch_size=len(test_ds))\n",
    "\n",
    "indim = 54\n",
    "hiddim = 20\n",
    "hiddim2 = 20\n",
    "outdim = 7\n",
    "\n",
    "learning_rate = 1e-3\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(indim,hiddim,bias=True),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hiddim,hiddim2,bias=True),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hiddim,outdim,bias=True),\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = len(train_dl.dataset)\n",
    "for epoch in range(epochs):\n",
    "    for batch, (X, y) in enumerate(train_dl):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        y_pred = torch.argmax(pred.data,1)\n",
    "        correct = (y == y_pred).sum().item()/len(y_pred)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch % 1 == 0) and (batch % 1000 == 0):\n",
    "            print(\"Epoch {} Batch {}: Training Loss = {}: Accuracy = {}\".format(epoch,batch,\n",
    "                                                        loss.item(),correct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for X, y in test_dl:\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        loss_value = loss.item()\n",
    "        \n",
    "        y_pred = torch.argmax(pred.data,1)\n",
    "        correct = (y == y_pred).sum().item()/len(y_pred)\n",
    "\n",
    "        print(\"Test Loss = {}: Accuracy = {}\".format(loss_value,correct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network classification on the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_openml('mnist_784', version=1, cache=True)\n",
    "mnist.target = mnist.target.astype(np.int8)\n",
    "X_cnn, y_cnn = mnist[\"data\"], mnist[\"target\"]\n",
    "X_train_cnn, X_test_cnn, y_train_cnn, y_test_cnn = train_test_split(X_cnn, y_cnn, test_size = 0.2,\n",
    "                                    shuffle = True, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cnn = X_train_cnn/255.\n",
    "X_test_cnn = X_test_cnn/255.\n",
    "\n",
    "X_trainf = torch.from_numpy(X_train_cnn.to_numpy()).float()\n",
    "X_testf = torch.from_numpy(X_test_cnn.to_numpy()).float()\n",
    "y_train = torch.from_numpy(y_train_cnn.to_numpy()).long()\n",
    "y_test = torch.from_numpy(y_test_cnn.to_numpy()).long()\n",
    "\n",
    "X_train = X_trainf.view(-1,1,28,28)\n",
    "X_test = X_testf.view(-1,1,28,28)\n",
    "plt.imshow(X_train[0][0], cmap='gray')\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = torch.nn.Conv2d(1,10,5)\n",
    "pool = torch.nn.MaxPool2d(2)\n",
    "conv2 = torch.nn.Conv2d(10,20,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[0:1].shape)\n",
    "print(conv1(X_train[0:1]).shape)\n",
    "print(pool(conv1(X_train[0:1])).shape)\n",
    "print(F.relu(pool(conv1(X_train[0:1]))).shape)\n",
    "print(conv2(F.relu(pool(conv1(X_train[0:1])))).shape)\n",
    "print(pool(conv2(F.relu(pool(conv1(X_train[0:1]))))).shape)\n",
    "\n",
    "imgout = F.relu(pool(conv2(F.relu(pool(conv1(X_train[0:1]))))))\n",
    "print(imgout.shape)\n",
    "print(np.prod(imgout.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 32\n",
    "train_ds = TensorDataset(X_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs)\n",
    "test_ds = TensorDataset(X_test, y_test)\n",
    "test_dl = DataLoader(test_ds, batch_size=len(test_ds))\n",
    "learning_rate = 1e-3\n",
    "epochs = 1\n",
    "\n",
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1,10,5)\n",
    "        self.pool = torch.nn.MaxPool2d(2)\n",
    "        self.conv2 = torch.nn.Conv2d(10,20,5)\n",
    "        self.fc1 = torch.nn.Linear(320,50)\n",
    "        self.fc2 = torch.nn.Linear(50,10)\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.pool(self.conv1(x)))\n",
    "        x = F.relu(self.pool(self.conv2(x)))\n",
    "        x = x.view(-1,320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "model = CNN()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = len(train_dl.dataset)\n",
    "for epoch in range(epochs):\n",
    "    for batch, (X, y) in enumerate(train_dl):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        y_pred = torch.argmax(pred.data,1)\n",
    "        correct = (y == y_pred).sum().item()/len(y_pred)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch % 1 == 0) and (batch % 100 == 0):\n",
    "            print(\"Epoch {} Batch {}: Training Loss = {}: Accuracy = {}\".format(epoch,batch,\n",
    "                                                        loss.item(),correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for X, y in test_dl:\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        loss_value = loss.item()\n",
    "        \n",
    "        y_pred = torch.argmax(pred.data,1)\n",
    "        correct = (y == y_pred).sum().item()/len(y_pred)\n",
    "\n",
    "        print(\"Test Loss = {}: Accuracy = {}\".format(loss_value,correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63239e487d98c7284e209f1e2423af1678d839c716ea06adfddffbe4e93e2bbb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
